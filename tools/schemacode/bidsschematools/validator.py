import datetime
import fnmatch
import json
import os
import re
from copy import deepcopy
from pathlib import Path

import bidsschematools as bst
import bidsschematools.schema
import bidsschematools.types
import bidsschematools.utils

lgr = bst.utils.get_logger()


def _get_paths(
    bids_paths,
    pseudofile_suffixes=[],
    dummy_paths=False,
    exclude_files=[],
):
    """
    Get all paths from a list of directories, excluding hidden subdirectories from distribution.

    Parameters
    ----------
    bids_paths : list or str
        Directories from which to get paths, may also contain file paths, which will remain
        unchanged.
    pseudofile_suffixes : list of str
        Directory suffixes prompting the validation of the directory name and limiting further
        directory walk.
    dummy_paths : bool, optional
        Whether to accept path strings which do not correspond to either files or directories.
    exclude_files : list, optional
        Files to exclude from listing.
        Dot files (`.*`) do not need to be explicitly listed, as these are excluded by default.

    Notes
    -----
    * Figure out how to return paths from BIDS root.
    * Deduplicate paths (if input dirs are subsets of other input dirs), might best be done at the
        very end.
    * Currently this only supports file entries (no directories of `**`) for `.bidsignore`
    """

    path_list = []
    bidsignore_list = []
    bids_root_found = False
    for bids_path in bids_paths:
        if not dummy_paths:
            bids_path = os.path.abspath(os.path.expanduser(bids_path))
        if os.path.isdir(bids_path):
            for root, dirs, file_names in os.walk(bids_path, topdown=True):
                if "dataset_description.json" in file_names:
                    if bids_root_found:
                        # Not currently supporting nested BIDS,
                        # do not index the contents of the directory.
                        dirs[:] = []
                        file_names[:] = []
                    else:
                        try:
                            with open(os.path.join(root, ".bidsignore")) as f:
                                bidsignore_list = f.read().splitlines()
                        except FileNotFoundError:
                            pass
                        bids_root_found = True
                if root.endswith(tuple(pseudofile_suffixes)):
                    # Add the directory name to the validation paths list.
                    path_list.append(Path(root).as_posix() + "/")
                    # Do not index the contents of the directory.
                    dirs[:] = []
                    file_names[:] = []
                if os.path.basename(root).startswith("."):
                    dirs[:] = []
                    file_names[:] = []
                for file_name in file_names:
                    if file_name in exclude_files or file_name.startswith("."):
                        continue
                    if bidsignore_list:
                        ignored = False
                        for ignore_expression in bidsignore_list:
                            if fnmatch.fnmatch(file_name, ignore_expression):
                                ignored = True
                                break
                        if ignored:
                            continue
                    file_path = os.path.join(root, file_name)
                    # This will need to be replaced with bids root finding.
                    path_list.append(Path(file_path).as_posix())
        elif os.path.isfile(bids_path) or dummy_paths:
            path_list.append(Path(bids_path).as_posix())
        else:
            raise FileNotFoundError(
                f"The input path `{bids_path}` could not be located. If this is a string "
                "intended for path validation which does not correspond to an actual "
                "path, please set the `dummy_paths` parameter to True."
            )

    return path_list


def validate_all(
    paths_list,
    regex_schema,
):
    """
    Validate `bids_paths` based on a `regex_schema` dictionary list, including regexes.

    Parameters
    ----------
    bids_paths : list or str
        A string pointing to a BIDS directory for which paths should be validated, or a list
        of strings pointing to individual files or subdirectories which *all* reside within
        one and only one BIDS directory root (i.e. nested datasets should be validated
        separately).
    regex_schema : list of dict
        A list of dictionaries as generated by `regexify_all()`.

    Returns
    -------
    results : dict
        A dictionary reporting the target files for validation, the unmatched files and unmatched
        regexes, and optionally the itemwise comparison results.
        Keys include "schema_tracking", "path_tracking", "path_listing", "match_listing", and
        optionally "itemwise"

    Notes
    -----
    * Multi-source validation could be accomplished by distributing the resulting tracking_schema
        dictionary and further eroding it.
    * Currently only entities are captured in named groups, edit `load_top_level()` to name other
        groups as well.
    """

    tracking_schema = deepcopy(regex_schema)
    tracking_paths = deepcopy(paths_list)
    itemwise_results = []
    matched = False
    match_listing = []
    for target_path in paths_list:
        lgr.debug("Checking file `%s`.", target_path)
        lgr.debug("Trying file types:")
        for regex_entry in tracking_schema:
            target_regex = regex_entry["regex"]
            lgr.debug("\t* `%s`, with pattern: `%`", target_path, target_regex)
            matched = re.match(r"(?:.*/)?" + target_regex, target_path)
            itemwise_result = {}
            itemwise_result["path"] = target_path
            itemwise_result["regex"] = target_regex
            if matched:
                lgr.debug("Match identified.")
                itemwise_result["match"] = True
                itemwise_results.append(itemwise_result)
                break
            itemwise_result["match"] = False
            itemwise_results.append(itemwise_result)
        if matched:
            tracking_paths.remove(target_path)
            # Might be fragile since it relies on where the loop broke:
            if regex_entry["mandatory"]:
                tracking_schema.remove(regex_entry)
            match_entry = matched.groupdict()
            match_entry["path"] = target_path
            match_listing.append(match_entry)
        else:
            lgr.debug(
                "The `%s` file could not be matched to any regex schema entry.",
                target_path,
            )
    results = {}
    results["itemwise"] = itemwise_results
    results["schema_tracking"] = tracking_schema
    results["schema_listing"] = regex_schema
    results["path_tracking"] = tracking_paths
    results["path_listing"] = paths_list
    results["match_listing"] = match_listing

    return results


def write_report(
    validation_result,
    report_path="~/.cache/bidsschematools/validator-report_{datetime}-{pid}.log",
    datetime_format="%Y%m%d%H%M%SZ",
):
    """Write a human-readable report based on the validation result.

    Parameters
    ----------
    validation_result : dict
        A dictionary as returned by `validate_all()` with keys including "schema_tracking",
        "path_tracking", "path_listing", and, optionally "itemwise".
        The "itemwise" value, if present, should be a list of dictionaries, with keys including
        "path", "regex", and "match".
    report_path : str, optional
        A path under which the report is to be saved, `datetime`, and `pid`
        are available as variables for string formatting, and will be expanded to the
        current datetime (as per the `datetime_format` parameter)
        and process ID, respectively.
    datetime_format : str, optional
        A datetime format, optionally used for the report path.

    Notes
    -----
    * Not using f-strings in order to prevent arbitrary code execution.
    """

    report_path = report_path.format(
        datetime=datetime.datetime.utcnow().strftime(datetime_format),
        pid=os.getpid(),
    )
    report_path = os.path.abspath(os.path.expanduser(report_path))
    report_dir = os.path.dirname(report_path)
    try:
        os.makedirs(report_dir)
    except OSError:
        pass

    total_file_count = len(validation_result["path_listing"])
    validated_files_count = total_file_count - len(validation_result["path_tracking"])
    with open(report_path, "w") as f:
        try:
            for comparison in validation_result["itemwise"]:
                if comparison["match"]:
                    comparison_result = "A MATCH"
                else:
                    comparison_result = "no match"
                f.write(
                    f'- Comparing the `{comparison["path"]}` path to the `{comparison["regex"]}` '
                    f"pattern resulted in {comparison_result}.\n"
                )
        except KeyError:
            pass
        f.write(
            f"\nSUMMARY:\n{validated_files_count} out of {total_file_count} files were "
            "successfully validated, using the following regular expressions:"
        )
        for regex_entry in validation_result["schema_listing"]:
            f.write(f'\n\t- `{regex_entry["regex"]}`')
        f.write("\n")
        if len(validation_result["path_tracking"]) > 0:
            f.write("The following files were not matched by any regex schema entry:")
            f.write("\n\t* `")
            f.write("`\n\t* `".join(validation_result["path_tracking"]))
        else:
            f.write("All files were matched by a regex schema entry.")
        if len(validation_result["schema_tracking"]) > 0:
            f.write("\nThe following mandatory regex schema entries did not match any files:")
            f.write("\n")
            for entry in validation_result["schema_tracking"]:
                if entry["mandatory"]:
                    f.write(f'\t** `{entry["regex"]}`\n')
        else:
            f.write("All mandatory BIDS files were found.\n")
        f.close()
    lgr.info("BIDS validation log written to %s", report_path)


def _find_dataset_description(my_path):
    candidate = os.path.join(my_path, "dataset_description.json")
    # Windows support... otherwise we could do `if my_path == "/"`.
    if my_path == "/" or not any(i in my_path for i in ["/", "\\"]):
        return None
    if os.path.isfile(candidate):
        return candidate
    else:
        level_up = os.path.dirname(my_path.rstrip("/\\"))
        return _find_dataset_description(level_up)


def select_schema_dir(
    bids_root,
    schema_reference_root,
    schema_version,
    schema_min_version,
):
    """
    Select schema directory, according to a fallback logic whereby the schema path is
    either (1) `schema_version` if the value is a path, (2) a concatenation of
    `schema_reference_root` and `schema_version`, (3) a concatenation of the detected
    version specification from a `dataset_description.json` file if one is found in
    parents of the input path, (4) `schema_min_version` if no other version can be found
    or if the detected version from `dataset_description.json` is smaller than
    `schema_min_version`.

    Parameters
    ----------
    bids_paths : list of str
        Paths to be validated.
        Entries in this list will be used to crawl the directory tree upwards until a
        dataset_description.json file is found.
    schema_reference_root : str, optional
        Path where schema versions are stored, and which contains directories named exactly
        according to the respective schema version, e.g. "1.7.0".
        If the path starts with the string "{module_path}" it will be expanded relative to the
        module path.
    schema_version : str or None
        Version of BIDS schema, or path to schema.
        If a path is given, this will be expanded and used directly, not concatenated with
        `schema_reference_root`.
        If the path starts with the string "{module_path}" it will be expanded relative to the
        module path.
        If None, the `dataset_description.json` fie will be queried for the dataset schema version.
    schema_min_version : str
        Minimal version to use UNLESS the schema version is manually specified.
        If the version is auto-detected and the version is smaller than schema_min_version,
        schema_min_version will be selected instead.


    Returns
    -------

    """
    # Expand module_path
    module_path = os.path.abspath(os.path.dirname(__file__))
    if schema_reference_root.startswith("{module_path}"):
        schema_reference_root = schema_reference_root.format(module_path=module_path)
    schema_reference_root = os.path.abspath(os.path.expanduser(schema_reference_root))

    # Handle path schema specification
    if schema_version:
        if "/" in schema_version:
            schema_dir = schema_version
            if schema_version.startswith("{module_path}"):
                schema_dir = schema_version.format(module_path=module_path)
            schema_dir = os.path.abspath(os.path.expanduser(schema_dir))
            return schema_dir
        schema_dir = os.path.join(schema_reference_root, schema_version)
        return schema_dir

    if bids_root:
        dataset_description = os.path.join(bids_root, "dataset_description.json")
        with open(dataset_description) as f:
            try:
                dataset_info = json.load(f)
            except json.decoder.JSONDecodeError:
                lgr.error(
                    "The `%s` file could not be loaded. "
                    "Please check whether the file is valid JSON. "
                    "Falling back to the `%s` BIDS version.",
                    dataset_description,
                    schema_min_version,
                )
                schema_version = schema_min_version
            else:
                try:
                    schema_version = dataset_info["BIDSVersion"]
                except KeyError:
                    lgr.warning(
                        "BIDSVersion is not specified in "
                        "`dataset_description.json`. "
                        "Falling back to `%s`.",
                        schema_min_version,
                    )
                    schema_version = schema_min_version
    if not schema_version:
        lgr.warning(
            "No BIDSVersion could be determined for the dataset. Falling back to `%s`.",
            schema_min_version,
        )
        schema_version = schema_min_version
    elif schema_min_version:
        if schema_version < schema_min_version:
            lgr.warning(
                "BIDSVersion `%s` is less than the minimal working "
                "`%s`. "
                "Falling back to `%s`. "
                "To force the usage of earlier versions specify them explicitly "
                "when calling the validator.",
                schema_version,
                schema_min_version,
                schema_min_version,
            )
            schema_version = schema_min_version
    schema_dir = os.path.join(schema_reference_root, schema_version)
    if os.path.isdir(schema_dir):
        return schema_dir
    else:
        raise ValueError(
            f"The expected schema directory {schema_dir} does not exist on the system. "
            "Please ensure the file exists or manually specify a schema version for "
            "which the bidsschematools files are available on your system."
        )


def log_errors(validation_result):
    """
    Raise errors for validation result.

    Parameters
    ----------
    validation_result : dict
        A dictionary as returned by `validate_all()` with keys including "schema_tracking",
        "path_tracking", "path_listing", and, optionally "itemwise".
        The "itemwise" value, if present, should be a list of dictionaries, with keys including
        "path", "regex", and "match".
    """
    total_file_count = len(validation_result["path_listing"])
    validated_files_count = total_file_count - len(validation_result["path_tracking"])
    if validated_files_count == 0:
        lgr.error("No valid BIDS files were found.")
    for entry in validation_result["schema_tracking"]:
        if entry["mandatory"]:
            lgr.error(
                "The `%s` regex pattern file required by BIDS was not found.",
                entry["regex"],
            )
    for i in validation_result["path_tracking"]:
        lgr.warning("The `%s` file was not matched by any regex schema entry.", i)


def _get_directory_suffixes(my_schema):
    """Query schema for suffixes which identify directory entities.

    Parameters
    ----------
    my_schema : dict
        Nested directory as produced by `bidsschematools.schema.load_schema()`.

    Returns
    -------
    list of str
        Directory pseudofile suffixes excluding trailing slashes.

    Notes
    -----
    * Yes this seems super-awkward to do explicitly, after all, the trailing slash is
        already in so it should automagically work, but no:
        - Subdirectory names need to be dynamically excluded from validation input.
        - Backslash directory delimiters are still in use, which is regrettable.
    """
    pseudofile_suffixes = []
    for i in my_schema["objects"]["extensions"].values():
        i_value = i["value"]
        if i_value.endswith("/") and i_value != "/":
            pseudofile_suffixes.append(i_value[:-1])
    return pseudofile_suffixes


def validate_bids(
    in_paths,
    dummy_paths=False,
    schema_reference_root="{module_path}/data/",
    schema_version=None,
    report_path=False,
    suppress_errors=False,
    schema_min_version="schema",
    accept_non_bids_dir=False,
    exclude_files=[],
):
    """
    Validate paths according to BIDS schema.

    Parameters
    ----------
    in_paths : str or list of str
        Paths which to validate, may be individual files or directories.
    dummy_paths : bool, optional
        Whether to accept path strings which do not correspond to either files or directories.
    schema_reference_root : str, optional
        Path where schema versions are stored, and which contains directories named exactly
        according to the respective schema version, e.g. "1.7.0".
        If the path starts with the string "{module_path}" it will be expanded relative to the
        module path.
    schema_version : str or None, optional
        Version of BIDS schema, or path to schema.
        If a path is given, this will be expanded and used directly, not concatenated with
        `schema_reference_root`.
        If the path starts with the string "{module_path}" it will be expanded relative to the
        module path.
        If None, the `dataset_description.json` fie will be queried for the dataset schema version.
    report_path : bool or str, optional
        If `True` a log will be written using the standard output path of `.write_report()`.
        If string, the string will be used as the output path.
        If the variable evaluates as False, no log will be written.
    schema_min_version : str, optional
        Minimal working schema version, used by the `bidsschematools.select_schema_dir()` function
        only if no schema version is found or a lower schema version is specified by the dataset.
    accept_non_bids_dir : bool, optional
    exclude_files : str, optional
        Files which will not be indexed for validation, use this if your data is in an archive
        standard which requires the presence of archive-specific files (e.g. DANDI requiring
        `dandiset.yaml`).
        Dot files (`.*`) do not need to be explicitly listed, as these are excluded by default.

    Returns
    -------
    results : dict
        A dictionary reporting the target files for validation, the unmatched files and unmatched
        regexes, and optionally the itemwise comparison results.
        Keys include "schema_tracking", "path_tracking", "path_listing", "match_listing", and
        optionally "itemwise"

    Examples
    --------

    ::

        from bidsschematools import validator
        bids_paths = '~/.data2/datalad/000026/rawdata'
        schema_version='{module_path}/data/schema/'
        validator.validate_bids(bids_paths, schema_version=schema_version)

    Notes
    -----
    * Needs to account for inheritance principle, probably somewhere deeper in the logic, might be
        as simple as pattern parsing and multiplying patterns to which inheritance applies.
        https://github.com/bids-standard/bids-specification/pull/969#issuecomment-1132119492
    """

    if isinstance(in_paths, str):
        in_paths = [in_paths]

    if dummy_paths:
        bids_root = None
    else:
        bids_root = _find_bids_root(in_paths, accept_non_bids_dir)

    bids_schema_dir = select_schema_dir(
        bids_root,
        schema_reference_root,
        schema_version,
        schema_min_version=schema_min_version,
    )
    regex_schema, my_schema = bst.parse.regexify_all(bids_schema_dir)
    pseudofile_suffixes = _get_directory_suffixes(my_schema)
    bids_paths = _get_paths(
        in_paths,
        dummy_paths=dummy_paths,
        pseudofile_suffixes=pseudofile_suffixes,
        exclude_files=exclude_files,
    )
    validation_result = validate_all(
        bids_paths,
        regex_schema,
    )

    # Record schema version.
    bids_version = bst.schema._get_bids_version(bids_schema_dir)
    validation_result["bids_version"] = bids_version

    log_errors(validation_result)

    if report_path:
        if isinstance(report_path, str):
            write_report(validation_result, report_path=report_path)
        else:
            write_report(validation_result)

    return validation_result


def _find_bids_root(in_paths, accept_non_bids_dir):
    """
    Return BIDS root for a list of paths.
    Raise error if more than one root is found and, optionally, if none is found.
    """

    dataset_descriptions = []
    for in_path in in_paths:
        in_path = os.path.abspath(os.path.expanduser(in_path))
        dataset_description = _find_dataset_description(in_path)
        if dataset_description and dataset_description not in dataset_descriptions:
            dataset_descriptions.append(dataset_description)
    if len(dataset_descriptions) > 1:
        raise ValueError(
            f"You have selected files belonging to {len(dataset_descriptions)} "
            "different datasets. Please run the validator once per dataset."
        )
    elif len(dataset_descriptions) == 0:
        if accept_non_bids_dir:
            lgr.warning(
                "None of the files in the input list are part of a BIDS dataset. Proceeding."
            )
            return ""
        else:
            raise ValueError(
                "None of the files in the input list are part of a BIDS dataset. Aborting."
            )
    return os.path.dirname(dataset_descriptions[0])
